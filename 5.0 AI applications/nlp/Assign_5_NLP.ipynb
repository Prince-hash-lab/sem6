{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMqpaBoBQqu6"
   },
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBdpjN0LQqu7"
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3M_28uvQqu8"
   },
   "source": [
    "Another popular text analysis technique is called topic modeling. The ultimate goal of topic modeling is to find various topics that are present in your corpus. Each document in the corpus will be made up of at least one topic, if not multiple topics.\n",
    "\n",
    "In this notebook, we will be covering the steps on how to do **Latent Dirichlet Allocation (LDA)**, which is one of many topic modeling techniques. It was specifically designed for text data.\n",
    "\n",
    "To use a topic modeling technique, you need to provide (1) a document-term matrix and (2) the number of topics you would like the algorithm to pick up.\n",
    "\n",
    "Once the topic modeling technique is applied, your job as a human is to interpret the results and see if the mix of words in each topic make sense. If they don't make sense, you can try changing up the number of topics, the terms in the document-term matrix, model parameters, or even try a different model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in g:\\softwares\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in g:\\softwares\\lib\\site-packages (from scipy) (1.20.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9F8sFpU4Qqu8"
   },
   "source": [
    "## Topic Modeling - Attempt #1 (All Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "EpY6VReue9az"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "id": "7r8T_M4nQqu9",
    "outputId": "ebfa66fa-c7ef-4ecb-fdf1-35e11edbd822"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaaaah</th>\n",
       "      <th>aaah</th>\n",
       "      <th>aah</th>\n",
       "      <th>abc</th>\n",
       "      <th>abcs</th>\n",
       "      <th>ability</th>\n",
       "      <th>abject</th>\n",
       "      <th>able</th>\n",
       "      <th>ablebodied</th>\n",
       "      <th>abled</th>\n",
       "      <th>...</th>\n",
       "      <th>zero</th>\n",
       "      <th>zillion</th>\n",
       "      <th>zip</th>\n",
       "      <th>ziplining</th>\n",
       "      <th>zipper</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zoning</th>\n",
       "      <th>zoo</th>\n",
       "      <th>éclair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ricky</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jim-G</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jim-J</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>George</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthony</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mike</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joe</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 7468 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         aaaaah  aaah  aah  abc  abcs  ability  abject  able  ablebodied  \\\n",
       "louis         0     0    3    0     0        0       0     1           0   \n",
       "dave          0     1    0    0     0        0       0     0           0   \n",
       "ricky         0     0    0    0     0        1       1     2           0   \n",
       "Jim-G         0     0    0    0     0        0       0     3           0   \n",
       "bill          1     0    0    0     1        0       0     1           0   \n",
       "Jim-J         0     0    0    0     0        0       0     1           2   \n",
       "john          0     0    0    0     0        0       0     3           0   \n",
       "George        0     0    0    0     0        0       0     2           0   \n",
       "ali           0     0    0    1     0        0       0     2           0   \n",
       "anthony       0     0    0    0     0        0       0     0           0   \n",
       "mike          0     0    0    0     0        0       0     0           0   \n",
       "joe           0     0    0    0     0        0       0     2           0   \n",
       "\n",
       "         abled  ...  zero  zillion  zip  ziplining  zipper  zombie  zombies  \\\n",
       "louis        0  ...     2        0    0          0       0       0        0   \n",
       "dave         0  ...     0        0    0          0       0       0        0   \n",
       "ricky        0  ...     0        0    0          0       0       0        0   \n",
       "Jim-G        0  ...     0        0    0          3       0       0        0   \n",
       "bill         0  ...     1        1    0          0       0       1        1   \n",
       "Jim-J        0  ...     0        0    0          0       0       0        0   \n",
       "john         0  ...     0        0    0          0       0       0        0   \n",
       "George       2  ...     0        0    1          0       1       0        0   \n",
       "ali          0  ...     0        0    0          0       0       1        0   \n",
       "anthony      0  ...     0        0    0          0       0       0        0   \n",
       "mike         0  ...     1        0    0          0       0       0        0   \n",
       "joe          0  ...     0        0    0          0       0       0        0   \n",
       "\n",
       "         zoning  zoo  éclair  \n",
       "louis         0    0       0  \n",
       "dave          0    0       0  \n",
       "ricky         0    1       0  \n",
       "Jim-G         0    0       0  \n",
       "bill          1    0       0  \n",
       "Jim-J         0    0       0  \n",
       "john          0    0       1  \n",
       "George        0    0       0  \n",
       "ali           0    0       0  \n",
       "anthony       0    0       0  \n",
       "mike          0    0       0  \n",
       "joe           0    0       0  \n",
       "\n",
       "[12 rows x 7468 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's read in our document-term matrix\n",
    "data = pd.read_pickle('dtm_prince1.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "gW0JnoKCQqu-"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_404/2732784014.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Import the necessary modules for LDA with gensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Terminal / Anaconda Navigator: conda install -c conda-forge gensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules for LDA with gensim\n",
    "# Terminal / Anaconda Navigator: conda install -c conda-forge gensim\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "wyZBkF0oQqu-",
    "outputId": "3df3c733-ec52-4e1d-e75d-d30aece6df51"
   },
   "outputs": [],
   "source": [
    "# One of the required inputs is a term-document matrix\n",
    "tdm = data.transpose()\n",
    "tdm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "0RLXUiNvQqu-"
   },
   "outputs": [],
   "source": [
    "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "8-LLkafYQqu_",
    "outputId": "c4810afa-5b7f-4a09-e813-704053061f5a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:338: UserWarning: Trying to unpickle estimator CountVectorizer from version 0.24.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "cv = pickle.load(open(\"cv_prince1.pkl\", \"rb\"))\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ESIM7bOCQqu_",
    "outputId": "946d17f4-06f0-471f-fc77-1913d43a6f89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 \n",
      " 0.031*\"level\" + 0.016*\"kindness\" + 0.016*\"hung\" + 0.015*\"journal\" + 0.014*\"dolphin\" + 0.011*\"reunion\" + 0.009*\"telling\" + 0.008*\"yesterday\" + 0.008*\"ruin\" + 0.007*\"paws\" \n",
      "\n",
      "Topic 1 \n",
      " 0.016*\"level\" + 0.012*\"paws\" + 0.011*\"dolphin\" + 0.009*\"journal\" + 0.008*\"hung\" + 0.008*\"telling\" + 0.007*\"kindness\" + 0.007*\"terrified\" + 0.006*\"chuck\" + 0.006*\"goatcheese\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10, random_state=np.random.RandomState(seed=10))\n",
    "\n",
    "for topic, topwords in lda.show_topics():\n",
    "    print(\"Topic\", topic, \"\\n\", topwords, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "stitEJzrQqvA",
    "outputId": "d14c909a-a0d1-40e9-b6b4-00d180a17704"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 \n",
      " 0.030*\"level\" + 0.017*\"hung\" + 0.016*\"kindness\" + 0.015*\"journal\" + 0.014*\"dolphin\" + 0.012*\"reunion\" + 0.009*\"yesterday\" + 0.009*\"telling\" + 0.008*\"ruin\" + 0.007*\"paws\" \n",
      "\n",
      "Topic 1 \n",
      " 0.029*\"level\" + 0.012*\"journal\" + 0.012*\"dolphin\" + 0.011*\"paws\" + 0.009*\"kindness\" + 0.009*\"hung\" + 0.008*\"telling\" + 0.007*\"glaad\" + 0.006*\"chuck\" + 0.006*\"yesterday\" \n",
      "\n",
      "Topic 2 \n",
      " 0.024*\"level\" + 0.015*\"kindness\" + 0.015*\"hung\" + 0.014*\"journal\" + 0.013*\"dolphin\" + 0.009*\"reunion\" + 0.009*\"goatcheese\" + 0.009*\"telling\" + 0.008*\"sex\" + 0.007*\"paws\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for 3 topics \n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=10, random_state=np.random.RandomState(seed=10))\n",
    "\n",
    "for topic, topwords in lda.show_topics():\n",
    "    print(\"Topic\", topic, \"\\n\", topwords, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NkC4e88IQqvA",
    "outputId": "e8aae166-fda6-4b93-e0ab-aeab3b251708"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 \n",
      " 0.023*\"level\" + 0.012*\"kindness\" + 0.011*\"hung\" + 0.011*\"reunion\" + 0.010*\"dolphin\" + 0.010*\"journal\" + 0.009*\"ruin\" + 0.007*\"goatcheese\" + 0.007*\"telling\" + 0.006*\"sam\" \n",
      "\n",
      "Topic 1 \n",
      " 0.001*\"level\" + 0.001*\"journal\" + 0.001*\"kindness\" + 0.001*\"dolphin\" + 0.001*\"hung\" + 0.001*\"yesterday\" + 0.000*\"telling\" + 0.000*\"paws\" + 0.000*\"reunion\" + 0.000*\"glaad\" \n",
      "\n",
      "Topic 2 \n",
      " 0.029*\"level\" + 0.017*\"hung\" + 0.017*\"kindness\" + 0.013*\"dolphin\" + 0.013*\"journal\" + 0.009*\"ruin\" + 0.009*\"telling\" + 0.007*\"paws\" + 0.006*\"goatcheese\" + 0.006*\"testaments\" \n",
      "\n",
      "Topic 3 \n",
      " 0.031*\"level\" + 0.018*\"journal\" + 0.016*\"hung\" + 0.016*\"kindness\" + 0.016*\"dolphin\" + 0.013*\"reunion\" + 0.010*\"yesterday\" + 0.010*\"paws\" + 0.010*\"telling\" + 0.009*\"glaad\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for 4 topics\n",
    "\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=10, random_state=np.random.RandomState(seed=10))\n",
    "\n",
    "for topic, topwords in lda.show_topics():\n",
    "    print(\"Topic\", topic, \"\\n\", topwords, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AwTgYmHAQqvA"
   },
   "source": [
    "## Topic Modeling - Attempt #2 (Nouns Only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvzkT1HhQqvA"
   },
   "source": [
    "One popular trick is to look only at terms that are from one part of speech (only nouns, only adjectives, etc.). Check out the UPenn tag set: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Prince\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Error with downloaded zip file\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Prince\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AADODqzTQqvB",
    "outputId": "c411e8d4-e482-43e3-b6a1-363e67e907f3"
   },
   "outputs": [],
   "source": [
    "\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n",
    "    is_noun = lambda pos: pos[:2] == 'NN' # pos = part-of-speech\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8v8UEj0hQqvB"
   },
   "outputs": [],
   "source": [
    "# Read in the cleaned data, before the CountVectorizer step\n",
    "data_clean = pd.read_pickle('corpus_prince.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "SI8N99M3QqvC",
    "outputId": "7b79776b-082d-43a0-c5ef-12b941d4bc45"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-ced70653-bda1-458d-8b6d-9ca426df33f5\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>music lets lights lights thank i i place place...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>jokes living stare work profound train thought...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ricky</th>\n",
       "      <td>hello thank fuck thank im gon youre weve money...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jim-G</th>\n",
       "      <td>thank thank thank thank thats gon week im itll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>thank thank pleasure georgia area oasis i june...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jim-J</th>\n",
       "      <td>ladies gentlemen stage mr jim jefferies thank ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john</th>\n",
       "      <td>charm wit snl writer john mulaney marriage bee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>George</th>\n",
       "      <td>– state jersey thats i times times times i hel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>ladies gentlemen stage ali hi thank hello na s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthony</th>\n",
       "      <td>thank thank people i em i francisco city world...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mike</th>\n",
       "      <td>wow hey thanks look insane years everyone i id...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joe</th>\n",
       "      <td>ladies gentlemen joe fck thanks phone fckface ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ced70653-bda1-458d-8b6d-9ca426df33f5')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-ced70653-bda1-458d-8b6d-9ca426df33f5 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-ced70653-bda1-458d-8b6d-9ca426df33f5');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                                transcript\n",
       "louis    music lets lights lights thank i i place place...\n",
       "dave     jokes living stare work profound train thought...\n",
       "ricky    hello thank fuck thank im gon youre weve money...\n",
       "Jim-G    thank thank thank thank thats gon week im itll...\n",
       "bill     thank thank pleasure georgia area oasis i june...\n",
       "Jim-J    ladies gentlemen stage mr jim jefferies thank ...\n",
       "john     charm wit snl writer john mulaney marriage bee...\n",
       "George   – state jersey thats i times times times i hel...\n",
       "ali      ladies gentlemen stage ali hi thank hello na s...\n",
       "anthony  thank thank people i em i francisco city world...\n",
       "mike     wow hey thanks look insane years everyone i id...\n",
       "joe      ladies gentlemen joe fck thanks phone fckface ..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns = pd.DataFrame(data_clean.transcript.apply(nouns))\n",
    "data_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new document-term matrix using only nouns\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 554
    },
    "id": "QSJq3vuhQqvC",
    "outputId": "a995a79d-cf5e-4f3c-d388-b93189871876"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-d5e77255-9bf5-4016-8530-fa6ec5802ccb\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aah</th>\n",
       "      <th>abc</th>\n",
       "      <th>abcs</th>\n",
       "      <th>ability</th>\n",
       "      <th>abortion</th>\n",
       "      <th>abortions</th>\n",
       "      <th>abstract</th>\n",
       "      <th>abuse</th>\n",
       "      <th>accent</th>\n",
       "      <th>accents</th>\n",
       "      <th>...</th>\n",
       "      <th>ze</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zeeb</th>\n",
       "      <th>zeppelin</th>\n",
       "      <th>zillion</th>\n",
       "      <th>zip</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zoo</th>\n",
       "      <th>éclair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ricky</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jim-G</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jim-J</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>George</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthony</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mike</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joe</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 4680 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d5e77255-9bf5-4016-8530-fa6ec5802ccb')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-d5e77255-9bf5-4016-8530-fa6ec5802ccb button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-d5e77255-9bf5-4016-8530-fa6ec5802ccb');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "         aah  abc  abcs  ability  abortion  abortions  abstract  abuse  \\\n",
       "louis      3    0     0        0         0          0         0      0   \n",
       "dave       0    0     0        0         0          1         0      0   \n",
       "ricky      0    0     0        1         0          0         0      0   \n",
       "Jim-G      0    0     0        0         0          0         0      0   \n",
       "bill       0    0     1        0         0          0         0      0   \n",
       "Jim-J      0    0     0        0         0          0         0      0   \n",
       "john       0    0     0        0         0          0         0      0   \n",
       "George     0    0     0        0         1          0         1      0   \n",
       "ali        0    1     0        0         0          0         0      0   \n",
       "anthony    0    0     0        0         2          0         0      0   \n",
       "mike       0    0     0        0         0          0         0      0   \n",
       "joe        0    0     0        0         0          0         0      1   \n",
       "\n",
       "         accent  accents  ...  ze  zealand  zeeb  zeppelin  zillion  zip  \\\n",
       "louis         0        0  ...   0        0     0         0        0    0   \n",
       "dave          0        0  ...   0        0     0         0        0    0   \n",
       "ricky         0        0  ...   0        0     0         0        0    0   \n",
       "Jim-G         0        0  ...   0        0     0         0        0    0   \n",
       "bill          0        0  ...   1        0     0         0        1    0   \n",
       "Jim-J         4        0  ...   0        0     0         0        0    0   \n",
       "john          1        0  ...   0        0     0         0        0    0   \n",
       "George        0        0  ...   0        0     1         0        0    1   \n",
       "ali           0        0  ...   0        0     0         0        0    0   \n",
       "anthony       1        0  ...   0       10     0         0        0    0   \n",
       "mike          0        0  ...   0        0     0         2        0    0   \n",
       "joe           1        1  ...   0        0     0         0        0    0   \n",
       "\n",
       "         zombie  zombies  zoo  éclair  \n",
       "louis         0        0    0       0  \n",
       "dave          0        0    0       0  \n",
       "ricky         0        0    1       0  \n",
       "Jim-G         0        0    0       0  \n",
       "bill          1        1    0       0  \n",
       "Jim-J         0        0    0       0  \n",
       "john          0        0    0       1  \n",
       "George        0        0    0       0  \n",
       "ali           1        0    0       0  \n",
       "anthony       0        0    0       0  \n",
       "mike          0        0    0       0  \n",
       "joe           0        0    0       0  \n",
       "\n",
       "[12 rows x 4680 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-add the additional stop words since we are recreating the document-term matrix\n",
    "add_stop_words = ['like', 'im', 'know', 'just', 'dont', 'thats', 'right', 'people','youre', 'got', 'gonna', 'time', 'think', 'yeah', 'said']\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
    "\n",
    "\n",
    "# Recreate a document-term matrix with only nouns\n",
    "cv_nouns = CountVectorizer(stop_words=stop_words)\n",
    "data_cv_nouns = cv_nouns.fit_transform(data_nouns.transcript)\n",
    "data_dtm_nouns = pd.DataFrame(data_cv_nouns.toarray(), columns=cv_nouns.get_feature_names())\n",
    "data_dtm_nouns.index = data_nouns.index\n",
    "data_dtm_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "G-5Wvsh1QqvD"
   },
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpus_nouns = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtm_nouns.transpose()))\n",
    "# Create the vocabulary dictionary\n",
    "id2word_nouns = dict((v, k) for k, v in cv_nouns.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "89XE4ZEyQqvD",
    "outputId": "2aab5a3f-b45d-4b7e-bc24-773e094ecfe2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.009*\"lot\" + 0.009*\"shit\" + 0.007*\"things\" + 0.007*\"man\" + 0.006*\"fck\" + 0.006*\"life\" + 0.006*\"theyre\" + 0.005*\"thing\" + 0.005*\"house\" + 0.005*\"day\"'),\n",
       " (1,\n",
       "  '0.010*\"thing\" + 0.009*\"day\" + 0.008*\"cause\" + 0.007*\"hes\" + 0.007*\"way\" + 0.007*\"guy\" + 0.007*\"shit\" + 0.007*\"life\" + 0.007*\"man\" + 0.006*\"gon\"')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "lda_nouns = models.LdaModel(corpus=corpus_nouns, num_topics=2, id2word=id2word_nouns, passes=10)\n",
    "lda_nouns.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZcL84tY_QqvD",
    "outputId": "e529582f-1c86-4a75-f05f-ee57b421d4cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.009*\"lot\" + 0.007*\"things\" + 0.007*\"cause\" + 0.006*\"kind\" + 0.006*\"day\" + 0.006*\"thing\" + 0.006*\"shit\" + 0.005*\"way\" + 0.005*\"point\" + 0.005*\"dog\"'),\n",
       " (1,\n",
       "  '0.010*\"shit\" + 0.008*\"ahah\" + 0.008*\"man\" + 0.008*\"lot\" + 0.007*\"fuck\" + 0.007*\"joke\" + 0.006*\"money\" + 0.006*\"didnt\" + 0.005*\"day\" + 0.005*\"women\"'),\n",
       " (2,\n",
       "  '0.011*\"thing\" + 0.010*\"day\" + 0.009*\"cause\" + 0.008*\"life\" + 0.008*\"hes\" + 0.008*\"man\" + 0.007*\"gon\" + 0.007*\"kids\" + 0.007*\"guy\" + 0.007*\"theyre\"')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try topics = 3\n",
    "lda_nouns = models.LdaModel(corpus=corpus_nouns, num_topics=3, id2word=id2word_nouns, passes=10)\n",
    "lda_nouns.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GAmLEYDVQqvE",
    "outputId": "e611898f-dfe2-4ec1-f2a5-25c71c4a9e71"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.011*\"life\" + 0.011*\"thing\" + 0.010*\"shit\" + 0.009*\"guy\" + 0.009*\"cause\" + 0.009*\"hes\" + 0.008*\"gon\" + 0.008*\"way\" + 0.008*\"kind\" + 0.008*\"things\"'),\n",
       " (1,\n",
       "  '0.012*\"day\" + 0.009*\"shit\" + 0.009*\"thing\" + 0.009*\"lot\" + 0.008*\"man\" + 0.007*\"women\" + 0.007*\"cause\" + 0.006*\"way\" + 0.006*\"fuck\" + 0.006*\"guy\"'),\n",
       " (2,\n",
       "  '0.007*\"hes\" + 0.007*\"theyre\" + 0.007*\"joke\" + 0.007*\"thing\" + 0.006*\"gon\" + 0.006*\"day\" + 0.006*\"children\" + 0.006*\"cause\" + 0.006*\"kids\" + 0.005*\"id\"'),\n",
       " (3,\n",
       "  '0.020*\"fck\" + 0.006*\"house\" + 0.006*\"man\" + 0.004*\"girl\" + 0.004*\"kids\" + 0.004*\"world\" + 0.004*\"water\" + 0.003*\"fcking\" + 0.003*\"theyre\" + 0.003*\"shit\"')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 4 topics\n",
    "lda_nouns = models.LdaModel(corpus=corpus_nouns, num_topics=4, id2word=id2word_nouns, passes=10)\n",
    "lda_nouns.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VntNDWeOQqvE"
   },
   "source": [
    "## Topic Modeling - Attempt #3 (Nouns and Adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Qdh8UsLiQqvE"
   },
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "def nouns_adj(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
    "    return ' '.join(nouns_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "_hYiAgoZQqvE",
    "outputId": "b2412336-e782-4a16-c39b-701b6e8ff597"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-1d06c1a2-f518-4830-a71f-032f094ccadd\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>music lets lights lights thank much i i i nice...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>dirty jokes living stare most hard work profou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ricky</th>\n",
       "      <td>hello great thank fuck thank lovely welcome im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jim-G</th>\n",
       "      <td>thank gosh thank much thank much thank thats n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>right thank thank pleasure greater atlanta geo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jim-J</th>\n",
       "      <td>ladies gentlemen welcome stage mr jim jefferie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john</th>\n",
       "      <td>boyish charm sharp wit former snl writer john ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>George</th>\n",
       "      <td>january – state new brunswick new jersey yeah ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>ladies gentlemen welcome stage ali wong hi wel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthony</th>\n",
       "      <td>thank san francisco thank good people surprise...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mike</th>\n",
       "      <td>wow hey thanks hey seattle nice look crazy ins...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joe</th>\n",
       "      <td>ladies gentlemen joe fck san francisco thanks ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1d06c1a2-f518-4830-a71f-032f094ccadd')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-1d06c1a2-f518-4830-a71f-032f094ccadd button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-1d06c1a2-f518-4830-a71f-032f094ccadd');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                                transcript\n",
       "louis    music lets lights lights thank much i i i nice...\n",
       "dave     dirty jokes living stare most hard work profou...\n",
       "ricky    hello great thank fuck thank lovely welcome im...\n",
       "Jim-G    thank gosh thank much thank much thank thats n...\n",
       "bill     right thank thank pleasure greater atlanta geo...\n",
       "Jim-J    ladies gentlemen welcome stage mr jim jefferie...\n",
       "john     boyish charm sharp wit former snl writer john ...\n",
       "George   january – state new brunswick new jersey yeah ...\n",
       "ali      ladies gentlemen welcome stage ali wong hi wel...\n",
       "anthony  thank san francisco thank good people surprise...\n",
       "mike     wow hey thanks hey seattle nice look crazy ins...\n",
       "joe      ladies gentlemen joe fck san francisco thanks ..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns_adj = pd.DataFrame(data_clean.transcript.apply(nouns_adj))\n",
    "data_nouns_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FuR2_BoPQqvE",
    "outputId": "4bb4a6c7-e314-45e6-afe1-1ee506b57e60"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df\n",
    "cv_nouns_adj = CountVectorizer(stop_words=stop_words, max_df=.8) # Remove corpus-specific stop words with max_df, if occurs >80%\n",
    "data_cv_nouns_adj = cv_nouns_adj.fit_transform(data_nouns_adj.transcript)\n",
    "data_dtm_nouns_adj = pd.DataFrame(data_cv_nouns_adj.toarray(), columns=cv_nouns_adj.get_feature_names())\n",
    "data_dtm_nouns_adj.index = data_nouns_adj.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "TSYXAf8kQqvE"
   },
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpus_nouns_adj = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtm_nouns_adj.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2word_nouns_adj = dict((v, k) for k, v in cv_nouns_adj.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GbLxsPGkQqvF",
    "outputId": "281e4da7-65ef-4d08-8adb-2a30e5d84528"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.006*\"fck\" + 0.004*\"dude\" + 0.003*\"dog\" + 0.003*\"everybody\" + 0.003*\"stupid\" + 0.003*\"fcking\" + 0.003*\"wife\" + 0.003*\"mom\" + 0.003*\"uh\" + 0.002*\"husband\"'),\n",
       " (1,\n",
       "  '0.006*\"fuck\" + 0.006*\"fucking\" + 0.005*\"joke\" + 0.003*\"dude\" + 0.003*\"friend\" + 0.003*\"parents\" + 0.003*\"everybody\" + 0.003*\"wife\" + 0.003*\"gun\" + 0.003*\"son\"')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "lda_nouns_adj = models.LdaModel(corpus=corpus_nouns_adj, num_topics=2, id2word=id2word_nouns_adj, passes=10)\n",
    "lda_nouns_adj.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e2okMn0zQqvF",
    "outputId": "d20f2b0c-3c5e-4230-c9ae-893d97e35ab4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.006*\"dude\" + 0.006*\"fck\" + 0.005*\"everybody\" + 0.005*\"joke\" + 0.005*\"fuck\" + 0.004*\"jenny\" + 0.004*\"fucking\" + 0.003*\"fcking\" + 0.003*\"anthony\" + 0.003*\"phone\"'),\n",
       " (1,\n",
       "  '0.005*\"dog\" + 0.005*\"fucking\" + 0.004*\"wife\" + 0.004*\"fuck\" + 0.004*\"joke\" + 0.003*\"mom\" + 0.003*\"clinton\" + 0.003*\"food\" + 0.003*\"parents\" + 0.003*\"guns\"'),\n",
       " (2,\n",
       "  '0.008*\"ahah\" + 0.007*\"fuck\" + 0.005*\"husband\" + 0.004*\"dude\" + 0.004*\"gay\" + 0.004*\"ok\" + 0.004*\"asian\" + 0.004*\"fucking\" + 0.003*\"everybody\" + 0.003*\"pregnant\"')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 3 topics\n",
    "lda_nouns_adj = models.LdaModel(corpus=corpus_nouns_adj, num_topics=3, id2word=id2word_nouns_adj, passes=10)\n",
    "lda_nouns_adj.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I1i1KTb0QqvF",
    "outputId": "f7e6581c-ef7d-4291-d99c-731a8c44f123"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.009*\"ahah\" + 0.008*\"joke\" + 0.008*\"fuck\" + 0.006*\"anthony\" + 0.006*\"mad\" + 0.005*\"gay\" + 0.004*\"grandma\" + 0.004*\"fucking\" + 0.004*\"jokes\" + 0.004*\"friend\"'),\n",
       " (1,\n",
       "  '0.007*\"clinton\" + 0.006*\"jenny\" + 0.005*\"parents\" + 0.005*\"friend\" + 0.005*\"mom\" + 0.004*\"cow\" + 0.004*\"wife\" + 0.004*\"john\" + 0.004*\"phone\" + 0.003*\"uh\"'),\n",
       " (2,\n",
       "  '0.008*\"dude\" + 0.006*\"everybody\" + 0.006*\"fuck\" + 0.006*\"fck\" + 0.005*\"dog\" + 0.005*\"fucking\" + 0.004*\"ok\" + 0.003*\"dick\" + 0.003*\"stupid\" + 0.003*\"fcking\"'),\n",
       " (3,\n",
       "  '0.006*\"fucking\" + 0.006*\"joke\" + 0.004*\"fuck\" + 0.004*\"guns\" + 0.004*\"wife\" + 0.003*\"uh\" + 0.003*\"girlfriend\" + 0.003*\"cunt\" + 0.003*\"youve\" + 0.003*\"son\"')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 4 topics\n",
    "lda_nouns_adj = models.LdaModel(corpus=corpus_nouns_adj, num_topics=4, id2word=id2word_nouns_adj, passes=10)\n",
    "lda_nouns_adj.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GjnpzxYkQqvF"
   },
   "source": [
    "## Identify Topics in Each Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xtvM15-tQqvF"
   },
   "source": [
    "Out of the 9 topic models we looked at, the nouns and adjectives, 4 topic one made the most sense. So let's pull that down here and run it through some more iterations to get more fine-tuned topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LT8IfFBYQqvF",
    "outputId": "282500fa-7718-4a0a-c41d-fc39e6728288"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.006*\"dog\" + 0.005*\"fucking\" + 0.005*\"fuck\" + 0.004*\"joke\" + 0.004*\"dude\" + 0.003*\"everybody\" + 0.003*\"clinton\" + 0.003*\"mom\" + 0.003*\"wife\" + 0.003*\"nuts\"'),\n",
       " (1,\n",
       "  '0.008*\"fck\" + 0.008*\"fuck\" + 0.006*\"fucking\" + 0.006*\"ahah\" + 0.004*\"guns\" + 0.004*\"dude\" + 0.004*\"fcking\" + 0.004*\"son\" + 0.004*\"everybody\" + 0.003*\"dick\"'),\n",
       " (2,\n",
       "  '0.005*\"joke\" + 0.005*\"uh\" + 0.004*\"jenny\" + 0.004*\"friend\" + 0.004*\"anthony\" + 0.004*\"phone\" + 0.004*\"mom\" + 0.003*\"parents\" + 0.003*\"husband\" + 0.003*\"wife\"'),\n",
       " (3,\n",
       "  '0.000*\"nope\" + 0.000*\"lights\" + 0.000*\"neighborhood\" + 0.000*\"tragedy\" + 0.000*\"wipe\" + 0.000*\"responsible\" + 0.000*\"boston\" + 0.000*\"happens\" + 0.000*\"race\" + 0.000*\"comments\"')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our final LDA model (for now)\n",
    "\n",
    "ldana = models.LdaModel(corpus=corpus_nouns_adj, num_topics=4, id2word=id2word_nouns_adj, passes=80)\n",
    "\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ps7w4Sj_QqvH",
    "outputId": "03bb11ba-f394-4a74-a148-a18482b3e78d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'louis'),\n",
       " (1, 'dave'),\n",
       " (0, 'ricky'),\n",
       " (2, 'Jim-G'),\n",
       " (0, 'bill'),\n",
       " (1, 'Jim-J'),\n",
       " (0, 'john'),\n",
       " (0, 'George'),\n",
       " (2, 'ali'),\n",
       " (2, 'anthony'),\n",
       " (2, 'mike'),\n",
       " (1, 'joe')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at which topics each transcript contains\n",
    "corpus_transformed = ldana[corpus_nouns_adj]\n",
    "\n",
    "list(zip([a for [(a,b)] in corpus_transformed], data_dtm_nouns_adj.index))\n",
    "\n",
    "# corpus_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lNz9jL-YQqvH"
   },
   "source": [
    "### Assignment:\n",
    "1. Try further modifying the parameters of the topic models above and see if you can get better topics.\n",
    "2. Create a new topic model that includes terms from a different [part of speech](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) and see if you can get better topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3wOOWQg9eHF-"
   },
   "source": [
    "*Assignment*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "S8hw-zUYQqvI"
   },
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "def modal_adv(text):\n",
    "    is_modal_adv = lambda pos: pos[:2] == 'MD' or pos[:2] == 'RB'\n",
    "    tokenized = word_tokenize(text)\n",
    "    modal_adv = [word for (word, pos) in pos_tag(tokenized) if is_modal_adv(pos)] \n",
    "    return ' '.join(modal_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "w6H3GsKQXYFK",
    "outputId": "a4b8d7cd-0552-4581-9e69-c69ee55af887"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-7e911320-1820-4138-9fc7-77fd68149020\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>very necessarily very much well easily around ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>very much here can very im not ever detroit we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ricky</th>\n",
       "      <td>just not just well actually so not obviously t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jim-G</th>\n",
       "      <td>so so so almost probably so probably so just c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>very much here here here here ridiculously the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jim-J</th>\n",
       "      <td>very sweet right now very now so very really n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john</th>\n",
       "      <td>sly there will pretty again very now then so t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>George</th>\n",
       "      <td>regularly dont even together too here distinct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>so very very yes can because now dont even cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthony</th>\n",
       "      <td>so much so here would most politically not not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mike</th>\n",
       "      <td>here so ago pretty really would sometimes can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joe</th>\n",
       "      <td>im na not not can right not also not not yet j...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7e911320-1820-4138-9fc7-77fd68149020')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-7e911320-1820-4138-9fc7-77fd68149020 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-7e911320-1820-4138-9fc7-77fd68149020');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                                transcript\n",
       "louis    very necessarily very much well easily around ...\n",
       "dave     very much here can very im not ever detroit we...\n",
       "ricky    just not just well actually so not obviously t...\n",
       "Jim-G    so so so almost probably so probably so just c...\n",
       "bill     very much here here here here ridiculously the...\n",
       "Jim-J    very sweet right now very now so very really n...\n",
       "john     sly there will pretty again very now then so t...\n",
       "George   regularly dont even together too here distinct...\n",
       "ali      so very very yes can because now dont even cou...\n",
       "anthony  so much so here would most politically not not...\n",
       "mike     here so ago pretty really would sometimes can ...\n",
       "joe      im na not not can right not also not not yet j..."
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_modal_adv = pd.DataFrame(data_clean.transcript.apply(modal_adv))\n",
    "data_modal_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-1KJqXiZXZrQ",
    "outputId": "9f697a78-7632-4a41-ce62-7a66f82c8565"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df\n",
    "cv_modal_adv = CountVectorizer(stop_words=stop_words, max_df=.8) # Remove corpus-specific stop words with max_df, if occurs >80%\n",
    "data_cv_modal_adv = cv_modal_adv.fit_transform(data_modal_adv.transcript)\n",
    "data_dtm_modal_adv = pd.DataFrame(data_cv_modal_adv.toarray(), columns=cv_modal_adv.get_feature_names())\n",
    "data_dtm_modal_adv.index = data_modal_adv.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fw24W4iKXpUQ"
   },
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpus_modal_adv = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtm_modal_adv.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2word_modal_adv = dict((v, k) for k, v in cv_modal_adv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lOmUcg7uXpGV",
    "outputId": "67c351bb-6c2d-44a5-e5ad-05e6403c8c0b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.028*\"obviously\" + 0.025*\"oh\" + 0.018*\"finally\" + 0.016*\"ago\" + 0.016*\"yes\" + 0.016*\"basically\" + 0.016*\"soon\" + 0.014*\"recently\" + 0.014*\"suddenly\" + 0.014*\"immediately\"'),\n",
       " (1,\n",
       "  '0.029*\"ta\" + 0.024*\"yes\" + 0.018*\"especially\" + 0.018*\"immediately\" + 0.018*\"finally\" + 0.018*\"basically\" + 0.013*\"forever\" + 0.013*\"youve\" + 0.013*\"recently\" + 0.013*\"literally\"'),\n",
       " (2,\n",
       "  '0.035*\"ought\" + 0.031*\"pretty\" + 0.027*\"definitely\" + 0.025*\"totally\" + 0.025*\"ta\" + 0.024*\"exactly\" + 0.016*\"completely\" + 0.014*\"yes\" + 0.012*\"slowly\" + 0.012*\"youd\"'),\n",
       " (3,\n",
       "  '0.029*\"soon\" + 0.022*\"ta\" + 0.015*\"hard\" + 0.015*\"especially\" + 0.015*\"honestly\" + 0.015*\"laugh\" + 0.015*\"immediately\" + 0.015*\"exactly\" + 0.015*\"fun\" + 0.015*\"peacefully\"')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_modal_adv = models.LdaModel(corpus=corpus_modal_adv, num_topics=4, id2word=id2word_modal_adv, passes=80)\n",
    "lda_modal_adv.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c5MEBrTwbEHr",
    "outputId": "58d28a26-5854-4fe1-fe79-b97e81deee3e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.072*\"ta\" + 0.043*\"exactly\" + 0.037*\"definitely\" + 0.013*\"constantly\" + 0.013*\"slowly\" + 0.013*\"em\" + 0.013*\"youve\" + 0.013*\"eventually\" + 0.013*\"totally\" + 0.013*\"oneway\"'),\n",
       " (1,\n",
       "  '0.028*\"obviously\" + 0.017*\"recently\" + 0.017*\"ago\" + 0.017*\"yes\" + 0.017*\"pretty\" + 0.017*\"oh\" + 0.015*\"eventually\" + 0.015*\"suddenly\" + 0.015*\"totally\" + 0.015*\"finally\"'),\n",
       " (2,\n",
       "  '0.028*\"basically\" + 0.022*\"yes\" + 0.020*\"pretty\" + 0.020*\"oh\" + 0.018*\"finally\" + 0.018*\"ta\" + 0.016*\"immediately\" + 0.016*\"soon\" + 0.016*\"totally\" + 0.014*\"especially\"'),\n",
       " (3,\n",
       "  '0.090*\"ought\" + 0.021*\"definitely\" + 0.021*\"absolutely\" + 0.016*\"theyre\" + 0.016*\"suddenly\" + 0.016*\"completely\" + 0.011*\"seriously\" + 0.011*\"ahead\" + 0.011*\"especially\" + 0.011*\"usually\"')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our final LDA model (for now)\n",
    "ldana = models.LdaModel(corpus=corpus_modal_adv, num_topics=4, id2word=id2word_modal_adv, passes=80)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-KqyHuWjZf0E",
    "outputId": "986331e1-0a73-4bcc-c2af-18aeb4338d71"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 'louis'),\n",
       " (1, 'dave'),\n",
       " (1, 'ricky'),\n",
       " (1, 'Jim-G'),\n",
       " (2, 'bill'),\n",
       " (2, 'Jim-J'),\n",
       " (2, 'john'),\n",
       " (3, 'George'),\n",
       " (2, 'ali'),\n",
       " (1, 'anthony'),\n",
       " (1, 'mike'),\n",
       " (0, 'joe')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_transformed = ldana[corpus_modal_adv]\n",
    "list(zip([a for [(a,b)] in corpus_transformed], data_dtm_modal_adv.index))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assign 5 NLP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
