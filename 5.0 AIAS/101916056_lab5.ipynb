{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body.\n",
    "\n",
    "In this notebook, we will be covering the steps on how to do **Latent Dirichlet Allocation (LDA)**, which is one of many topic modeling techniques. It was specifically designed for text data.\n",
    "\n",
    "To use a topic modeling technique, you need to provide \n",
    "(1) a document-term matrix and \n",
    "(2) the number of topics you would like the algorithm to pick up.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>05</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>10000</th>\n",
       "      <th>100000</th>\n",
       "      <th>10abox</th>\n",
       "      <th>11</th>\n",
       "      <th>...</th>\n",
       "      <th>ze</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zeppelin</th>\n",
       "      <th>zero</th>\n",
       "      <th>zillion</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zoning</th>\n",
       "      <th>zoo</th>\n",
       "      <th>éclair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ricky</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bo</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jim</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hasan</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthony</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mike</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joe</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 6589 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         05  07  08  10  100  1000  10000  100000  10abox  11  ...  ze  \\\n",
       "louis     0   0   0   1    1     0      0       0       0   0  ...   0   \n",
       "dave      0   0   0   1    0     0      0       0       0   1  ...   0   \n",
       "ricky     0   0   0   0    0     0      0       2       0   0  ...   0   \n",
       "bo        0   0   0   0    0     0      0       0       0   0  ...   0   \n",
       "bill      1   1   1   1    1     0      0       0       0   1  ...   1   \n",
       "jim       0   0   0   4    0     2      0       0       0   0  ...   0   \n",
       "john      0   0   0   0    1     0      0       0       0   0  ...   0   \n",
       "hasan     0   0   0   0    0     0      0       0       0   0  ...   0   \n",
       "ali       0   0   0   0    1     0      0       0       1   0  ...   0   \n",
       "anthony   0   0   0   1    1     0      0       0       0   0  ...   0   \n",
       "mike      0   0   0   1    0     1      0       0       0   1  ...   0   \n",
       "joe       0   0   0   0    1     0      4       0       0   2  ...   0   \n",
       "\n",
       "         zealand  zeppelin  zero  zillion  zombie  zombies  zoning  zoo  \\\n",
       "louis          0         0     2        0       0        0       0    0   \n",
       "dave           0         0     0        0       0        0       0    0   \n",
       "ricky          0         0     0        0       0        0       0    1   \n",
       "bo             0         0     0        0       0        0       0    0   \n",
       "bill           0         0     1        1       1        1       1    0   \n",
       "jim            0         0     0        0       0        0       0    0   \n",
       "john           0         0     0        0       0        0       0    0   \n",
       "hasan          0         0     0        0       0        0       0    0   \n",
       "ali            0         0     0        0       1        0       0    0   \n",
       "anthony       10         0     0        0       0        0       0    0   \n",
       "mike           0         2     1        0       0        0       0    0   \n",
       "joe            0         0     0        0       0        0       0    0   \n",
       "\n",
       "         éclair  \n",
       "louis         0  \n",
       "dave          0  \n",
       "ricky         0  \n",
       "bo            0  \n",
       "bill          0  \n",
       "jim           0  \n",
       "john          1  \n",
       "hasan         0  \n",
       "ali           0  \n",
       "anthony       0  \n",
       "mike          0  \n",
       "joe           0  \n",
       "\n",
       "[12 rows x 6589 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's read in our document-term matrix\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "data = pd.read_pickle('dtm.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA will go through every word & its assigned topic and it will update the topic assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules for LDA with gensim\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>louis</th>\n",
       "      <th>dave</th>\n",
       "      <th>ricky</th>\n",
       "      <th>bo</th>\n",
       "      <th>bill</th>\n",
       "      <th>jim</th>\n",
       "      <th>john</th>\n",
       "      <th>hasan</th>\n",
       "      <th>ali</th>\n",
       "      <th>anthony</th>\n",
       "      <th>mike</th>\n",
       "      <th>joe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>05</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>07</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>08</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     louis  dave  ricky  bo  bill  jim  john  hasan  ali  anthony  mike  joe\n",
       "05       0     0      0   0     1    0     0      0    0        0     0    0\n",
       "07       0     0      0   0     1    0     0      0    0        0     0    0\n",
       "08       0     0      0   0     1    0     0      0    0        0     0    0\n",
       "10       1     1      0   0     1    4     0      0    0        1     1    0\n",
       "100      1     0      0   0     1    0     1      0    1        1     0    1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One of the required inputs is a term-document matrix\n",
    "tdm = data.transpose()\n",
    "tdm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the term-document matrix into a new gensim format, from df -> sparse matrix -> gensim corpus\n",
    "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4)\t1\n",
      "  (1, 4)\t1\n",
      "  (2, 4)\t1\n",
      "  (3, 0)\t1\n",
      "  (3, 1)\t1\n",
      "  (3, 4)\t1\n",
      "  (3, 5)\t4\n",
      "  (3, 9)\t1\n",
      "  (3, 10)\t1\n",
      "  (4, 0)\t1\n",
      "  (4, 4)\t1\n",
      "  (4, 6)\t1\n",
      "  (4, 8)\t1\n",
      "  (4, 9)\t1\n",
      "  (4, 11)\t1\n",
      "  (5, 5)\t2\n",
      "  (5, 10)\t1\n",
      "  (6, 11)\t4\n",
      "  (7, 2)\t2\n",
      "  (8, 8)\t1\n",
      "  (9, 1)\t1\n",
      "  (9, 4)\t1\n",
      "  (9, 10)\t1\n",
      "  (9, 11)\t2\n",
      "  (10, 1)\t1\n",
      "  :\t:\n",
      "  (6572, 6)\t1\n",
      "  (6573, 2)\t1\n",
      "  (6573, 11)\t1\n",
      "  (6574, 0)\t1\n",
      "  (6575, 0)\t1\n",
      "  (6575, 2)\t1\n",
      "  (6575, 4)\t1\n",
      "  (6575, 11)\t3\n",
      "  (6576, 2)\t1\n",
      "  (6577, 2)\t1\n",
      "  (6577, 4)\t1\n",
      "  (6578, 8)\t1\n",
      "  (6579, 4)\t1\n",
      "  (6580, 9)\t10\n",
      "  (6581, 10)\t2\n",
      "  (6582, 0)\t2\n",
      "  (6582, 4)\t1\n",
      "  (6582, 10)\t1\n",
      "  (6583, 4)\t1\n",
      "  (6584, 4)\t1\n",
      "  (6584, 8)\t1\n",
      "  (6585, 4)\t1\n",
      "  (6586, 4)\t1\n",
      "  (6587, 2)\t1\n",
      "  (6588, 6)\t1\n"
     ]
    }
   ],
   "source": [
    "print(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.matutils.Sparse2Corpus object at 0x0000021548EE61D0>\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cv = pickle.load(open(\"cv.pkl\", \"rb\"))\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have the corpus (term-document matrix) and id2word (dictionary of location: term), we need to specify two other parameters - the number of topics and the number of passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(stop_words='english')\n"
     ]
    }
   ],
   "source": [
    "print(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3094: 'introfade',\n",
       " 3882: 'music',\n",
       " 3394: 'let',\n",
       " 4941: 'roll',\n",
       " 2854: 'hold',\n",
       " 3425: 'lights',\n",
       " 5882: 'thank',\n",
       " 379: 'appreciate',\n",
       " 1832: 'don',\n",
       " 3933: 'necessarily',\n",
       " 248: 'agree',\n",
       " 3973: 'nice',\n",
       " 4375: 'place',\n",
       " 1947: 'easily',\n",
       " 3974: 'nicest',\n",
       " 3734: 'miles',\n",
       " 1749: 'direction',\n",
       " 1337: 'compliment',\n",
       " 895: 'building',\n",
       " 5224: 'shit',\n",
       " 6010: 'town',\n",
       " 5159: 'sentence',\n",
       " 4050: 'odd',\n",
       " 1892: 'driving',\n",
       " 1816: 'doesn',\n",
       " 1733: 'difference',\n",
       " 5277: 'sidewalk',\n",
       " 5633: 'street',\n",
       " 4268: 'pedestrians',\n",
       " 4287: 'people',\n",
       " 3218: 'just',\n",
       " 3261: 'kind',\n",
       " 6329: 'walk',\n",
       " 3728: 'middle',\n",
       " 4924: 'road',\n",
       " 3513: 'love',\n",
       " 6050: 'traveling',\n",
       " 5128: 'seeing',\n",
       " 1735: 'different',\n",
       " 4229: 'parts',\n",
       " 1445: 'country',\n",
       " 3452: 'live',\n",
       " 3964: 'new',\n",
       " 6569: 'york',\n",
       " 6244: 'value',\n",
       " 1820: 'doing',\n",
       " 4073: 'old',\n",
       " 3307: 'lady',\n",
       " 1817: 'dog',\n",
       " 3427: 'like',\n",
       " 3944: 'neighborhood',\n",
       " 6331: 'walking',\n",
       " 5543: 'stands',\n",
       " 2275: 'fights',\n",
       " 2625: 'gravity',\n",
       " 1601: 'day',\n",
       " 4738: 'really',\n",
       " 2595: 'got',\n",
       " 1248: 'cloudy',\n",
       " 2156: 'eye',\n",
       " 5540: 'stand',\n",
       " 3487: 'looking',\n",
       " 1743: 'dimensions',\n",
       " 6381: 'wearing',\n",
       " 5757: 'sweater',\n",
       " 1875: 'dress',\n",
       " 2661: 'guess',\n",
       " 3: '10',\n",
       " 3384: 'legs',\n",
       " 3980: 'nightmare',\n",
       " 6431: 'white',\n",
       " 2635: 'green',\n",
       " 5631: 'streaks',\n",
       " 769: 'bones',\n",
       " 5594: 'sticking',\n",
       " 514: 'awful',\n",
       " 5046: 'saw',\n",
       " 2672: 'guy',\n",
       " 6422: 'wheeling',\n",
       " 6555: 'yecch',\n",
       " 6343: 'want',\n",
       " 259: 'air',\n",
       " 3485: 'look',\n",
       " 5946: 'time',\n",
       " 5899: 'think',\n",
       " 2574: 'god',\n",
       " 2891: 'hope',\n",
       " 1731: 'dies',\n",
       " 5008: 'sake',\n",
       " 3497: 'lose',\n",
       " 3459: 'll',\n",
       " 157: 'able',\n",
       " 2712: 'handle',\n",
       " 6515: 'worried',\n",
       " 3287: 'know',\n",
       " 510: 'aware',\n",
       " 3010: 'inches',\n",
       " 2761: 'head',\n",
       " 3458: 'living',\n",
       " 6134: 'twosecond',\n",
       " 3021: 'increments',\n",
       " 5117: 'second',\n",
       " 3373: 'left',\n",
       " 3294: 'knows',\n",
       " 2583: 'gonna',\n",
       " 1695: 'destroyed',\n",
       " 4301: 'person',\n",
       " 3417: 'life',\n",
       " 4347: 'piece',\n",
       " 3450: 'little',\n",
       " 6571: 'young',\n",
       " 6485: 'woman',\n",
       " 3825: 'morning',\n",
       " 5049: 'say',\n",
       " 2584: 'good',\n",
       " 2560: 'gladys',\n",
       " 2366: 'flush',\n",
       " 5970: 'toilet',\n",
       " 4442: 'poom',\n",
       " 3230: 'keeps',\n",
       " 910: 'bumping',\n",
       " 1863: 'drain',\n",
       " 2556: 'gives',\n",
       " 2033: 'ends',\n",
       " 5229: 'shitting',\n",
       " 4859: 'rest',\n",
       " 4488: 'ppp',\n",
       " 1593: 'daughter',\n",
       " 3430: 'likes',\n",
       " 2310: 'fish',\n",
       " 4308: 'pet',\n",
       " 966: 'came',\n",
       " 2867: 'home',\n",
       " 1605: 'dead',\n",
       " 6360: 'wasn',\n",
       " 5068: 'school',\n",
       " 2367: 'flushed',\n",
       " 6400: 'weird',\n",
       " 3800: 'moment',\n",
       " 1037: 'cause',\n",
       " 6368: 'water',\n",
       " 2353: 'floats',\n",
       " 6551: 'yeah',\n",
       " 3488: 'looks',\n",
       " 2576: 'goes',\n",
       " 6323: 'wait',\n",
       " 506: 'aw',\n",
       " 5793: 'taking',\n",
       " 3906: 'nap',\n",
       " 3162: 'jesus',\n",
       " 2596: 'gotta',\n",
       " 1385: 'constant',\n",
       " 3841: 'motion',\n",
       " 4852: 'respect',\n",
       " 2917: 'house',\n",
       " 1299: 'comes',\n",
       " 1726: 'did',\n",
       " 1728: 'die',\n",
       " 1560: 'da',\n",
       " 1293: 'come',\n",
       " 4741: 'reason',\n",
       " 196: 'actual',\n",
       " 1727: 'didn',\n",
       " 3644: 'matter',\n",
       " 285: 'alive',\n",
       " 2169: 'facts',\n",
       " 5985: 'took',\n",
       " 386: 'aquarium',\n",
       " 3253: 'kids',\n",
       " 786: 'boston',\n",
       " 5106: 'seal',\n",
       " 5105: 'sea',\n",
       " 3438: 'lion',\n",
       " 995: 'care',\n",
       " 3937: 'need',\n",
       " 5161: 'separate',\n",
       " 5898: 'things',\n",
       " 5074: 'scientists',\n",
       " 6114: 'tv',\n",
       " 5978: 'tomorrow',\n",
       " 4071: 'ok',\n",
       " 2093: 'everybody',\n",
       " 5107: 'seals',\n",
       " 3439: 'lions',\n",
       " 6336: 'walruses',\n",
       " 4280: 'penguins',\n",
       " 4902: 'right',\n",
       " 2293: 'fine',\n",
       " 6562: 'yes',\n",
       " 3577: 'man',\n",
       " 5108: 'sealsea',\n",
       " 5897: 'thing',\n",
       " 4401: 'plexiglas',\n",
       " 2577: 'going',\n",
       " 1780: 'disgusting',\n",
       " 334: 'animals',\n",
       " 2202: 'farm',\n",
       " 5212: 'sheep',\n",
       " 522: 'baa',\n",
       " 2210: 'fat',\n",
       " 5225: 'shitcolored',\n",
       " 5052: 'saying',\n",
       " 5342: 'slave',\n",
       " 3254: 'kill',\n",
       " 2381: 'food',\n",
       " 1063: 'chain',\n",
       " 3266: 'kinds',\n",
       " 5675: 'stuff',\n",
       " 2559: 'glad',\n",
       " 4514: 'pretty',\n",
       " 1612: 'deal',\n",
       " 1378: 'consider',\n",
       " 302: 'alternatives',\n",
       " 2465: 'fully',\n",
       " 2166: 'fact',\n",
       " 3628: 'massive',\n",
       " 6207: 'upgrade',\n",
       " 1953: 'eaten',\n",
       " 5302: 'single',\n",
       " 2935: 'human',\n",
       " 152: 'aah',\n",
       " 4088: 'ones',\n",
       " 625: 'bed',\n",
       " 943: 'bye',\n",
       " 3662: 'mean',\n",
       " 2984: 'imagine',\n",
       " 2756: 'having',\n",
       " 540: 'bad',\n",
       " 6327: 'wake',\n",
       " 3572: 'making',\n",
       " 833: 'breakfast',\n",
       " 921: 'burn',\n",
       " 5963: 'toast',\n",
       " 3330: 'late',\n",
       " 6089: 'try',\n",
       " 3251: 'kid',\n",
       " 5053: 'says',\n",
       " 637: 'beh',\n",
       " 3562: 'mail',\n",
       " 4322: 'phone',\n",
       " 3830: 'mortgage',\n",
       " 1321: 'company',\n",
       " 6501: 'work',\n",
       " 788: 'bother',\n",
       " 2575: 'goddamn',\n",
       " 2456: 'fucking',\n",
       " 1117: 'cheetahs',\n",
       " 6028: 'train',\n",
       " 5566: 'station',\n",
       " 5612: 'stop',\n",
       " 5207: 'sharks',\n",
       " 2008: 'embarrassed',\n",
       " 3281: 'knew',\n",
       " 2300: 'fins',\n",
       " 908: 'bummed',\n",
       " 5352: 'slick',\n",
       " 5770: 'swim',\n",
       " 2813: 'hey',\n",
       " 2964: 'idea',\n",
       " 5995: 'totally',\n",
       " 5206: 'shark',\n",
       " 6524: 'wouldn',\n",
       " 2467: 'fun',\n",
       " 4496: 'predator',\n",
       " 2945: 'hungry',\n",
       " 1097: 'chase',\n",
       " 4990: 'running',\n",
       " 1915: 'dude',\n",
       " 704: 'bite',\n",
       " 3935: 'neck',\n",
       " 5944: 'till',\n",
       " 5267: 'shuts',\n",
       " 2451: 'fuck',\n",
       " 3752: 'minute',\n",
       " 5309: 'sit',\n",
       " 1952: 'eat',\n",
       " 5606: 'stomach',\n",
       " 5691: 'suck',\n",
       " 4211: 'parents',\n",
       " 512: 'away',\n",
       " 1183: 'circle',\n",
       " 1954: 'eating',\n",
       " 523: 'babies',\n",
       " 905: 'bullshit',\n",
       " 2653: 'grownups',\n",
       " 258: 'ain',\n",
       " 6520: 'worth',\n",
       " 3669: 'meat',\n",
       " 6439: 'whoops',\n",
       " 2972: 'idiot',\n",
       " 3116: 'isn',\n",
       " 2507: 'gay',\n",
       " 6305: 'voice',\n",
       " 6373: 'way',\n",
       " 5266: 'shut',\n",
       " 1456: 'courtyard',\n",
       " 1363: 'confrontation',\n",
       " 1189: 'city',\n",
       " 3453: 'lived',\n",
       " 2649: 'growing',\n",
       " 3806: 'money',\n",
       " 3799: 'mom',\n",
       " 1137: 'child',\n",
       " 6221: 'used',\n",
       " 2360: 'flowers',\n",
       " 2410: 'fountain',\n",
       " 3597: 'marble',\n",
       " 812: 'boys',\n",
       " 4369: 'pissing',\n",
       " 2411: 'fountains',\n",
       " 5102: 'sculptors',\n",
       " 4271: 'pedophiles',\n",
       " 591: 'basically',\n",
       " 3568: 'make',\n",
       " 5553: 'started',\n",
       " 2298: 'finished',\n",
       " 2162: 'face',\n",
       " 2634: 'greek',\n",
       " 3503: 'lot',\n",
       " 4367: 'piss',\n",
       " 2396: 'forever',\n",
       " 6393: 'week',\n",
       " 6553: 'year',\n",
       " 246: 'ago',\n",
       " 6406: 'went',\n",
       " 5713: 'sunday',\n",
       " 4504: 'presentable',\n",
       " 2915: 'hour',\n",
       " 5535: 'stains',\n",
       " 5314: 'sitting',\n",
       " 2677: 'ha',\n",
       " 5608: 'stone',\n",
       " 651: 'bench',\n",
       " 2238: 'feeling',\n",
       " 2193: 'fancy',\n",
       " 1840: 'doormen',\n",
       " 4009: 'notice',\n",
       " 5486: 'spiffylooking',\n",
       " 871: 'brown',\n",
       " 5236: 'shoes',\n",
       " 5843: 'tell',\n",
       " 5901: 'thinking',\n",
       " 5902: 'thinks',\n",
       " 6340: 'wandered',\n",
       " 5634: 'streets',\n",
       " 5030: 'sat',\n",
       " 1305: 'coming',\n",
       " 1615: 'dealing',\n",
       " 6090: 'trying',\n",
       " 2644: 'gross',\n",
       " 4608: 'pulling',\n",
       " 5222: 'shirt',\n",
       " 4021: 'num',\n",
       " 2109: 'excited',\n",
       " 6538: 'wrong',\n",
       " 4969: 'rrgh',\n",
       " 2112: 'excuse',\n",
       " 5007: 'said',\n",
       " 5552: 'start',\n",
       " 2733: 'hard',\n",
       " 4536: 'private',\n",
       " 4570: 'property',\n",
       " 643: 'believe',\n",
       " 6519: 'worst',\n",
       " 4416: 'point',\n",
       " 6286: 'view',\n",
       " 3367: 'leave',\n",
       " 5796: 'talk',\n",
       " 1839: 'doorman',\n",
       " 5569: 'stay',\n",
       " 2916: 'hours',\n",
       " 5798: 'talking',\n",
       " 3456: 'lives',\n",
       " 3891: 'mwah',\n",
       " 623: 'beautiful',\n",
       " 1266: 'cocktail',\n",
       " 329: 'anger',\n",
       " 1367: 'confusion',\n",
       " 3098: 'invented',\n",
       " 2949: 'hurt',\n",
       " 5431: 'somebody',\n",
       " 2239: 'feelings',\n",
       " 2538: 'getting',\n",
       " 103: '45',\n",
       " 2696: 'halfway',\n",
       " 2774: 'healthy',\n",
       " 4016: 'notsohealthy',\n",
       " 6488: 'won',\n",
       " 3480: 'long',\n",
       " 2127: 'expectancy',\n",
       " 4987: 'run',\n",
       " 1939: 'early',\n",
       " 6554: 'years',\n",
       " 5437: 'soon',\n",
       " 108: '50',\n",
       " 977: 'candidate',\n",
       " 979: 'candlelight',\n",
       " 6288: 'vigils',\n",
       " 113: '50yearold',\n",
       " 2673: 'guys',\n",
       " 4629: 'pushing',\n",
       " 139: '80s',\n",
       " 146: '90s',\n",
       " 11: '114',\n",
       " 3969: 'news',\n",
       " 3710: 'met',\n",
       " 3909: 'napoleon',\n",
       " 3407: 'liar',\n",
       " 4075: 'oldest',\n",
       " 6512: 'world',\n",
       " 3801: 'moments',\n",
       " 2075: 'especially',\n",
       " 6526: 'wow',\n",
       " 2729: 'happens',\n",
       " 6367: 'watching',\n",
       " 5695: 'sudden',\n",
       " 4734: 'realize',\n",
       " 6465: 'wipe',\n",
       " 437: 'ass',\n",
       " 2727: 'happened',\n",
       " 6076: 'trips',\n",
       " 598: 'bathroom',\n",
       " 1815: 'does',\n",
       " 2726: 'happen',\n",
       " 443: 'asshole',\n",
       " 6322: 'waistband',\n",
       " 4188: 'pajama',\n",
       " 793: 'bottoms',\n",
       " 3262: 'kinda',\n",
       " 3492: 'loose',\n",
       " 3029: 'ineffectual',\n",
       " 543: 'bag',\n",
       " 3368: 'leaves',\n",
       " 5938: 'tied',\n",
       " 3343: 'lawn',\n",
       " 4100: 'open',\n",
       " 4605: 'puking',\n",
       " 2623: 'grass',\n",
       " 6471: 'wisp',\n",
       " 6457: 'wind',\n",
       " 3249: 'kicks',\n",
       " 6001: 'tough',\n",
       " 4718: 'rats',\n",
       " 179: 'accurate',\n",
       " 1682: 'description',\n",
       " 237: 'age',\n",
       " 5542: 'standing',\n",
       " 663: 'better',\n",
       " 5845: 'tells',\n",
       " 4951: 'room',\n",
       " 5972: 'told',\n",
       " 3037: 'information',\n",
       " 2142: 'explain',\n",
       " 990: 'car',\n",
       " 6008: 'towed',\n",
       " 170: 'accept',\n",
       " 3665: 'means',\n",
       " 1627: 'decide',\n",
       " 360: 'anymore',\n",
       " 4935: 'rocking',\n",
       " 3802: 'momentum',\n",
       " 2876: 'honda',\n",
       " 5411: 'snow',\n",
       " 570: 'bank',\n",
       " 4634: 'putting',\n",
       " 5421: 'socks',\n",
       " 5861: 'terrible',\n",
       " 2478: 'future',\n",
       " 2616: 'grandmother',\n",
       " 3872: 'murdered',\n",
       " 2714: 'hands',\n",
       " 4242: 'past',\n",
       " 4417: 'pointed',\n",
       " 5969: 'toe',\n",
       " 2373: 'folding',\n",
       " 802: 'bowling',\n",
       " 552: 'ball',\n",
       " 2686: 'half',\n",
       " 2237: 'feel',\n",
       " 6302: 'vital',\n",
       " 4125: 'organs',\n",
       " 5782: 'systems',\n",
       " 2175: 'failing',\n",
       " 631: 'beep',\n",
       " 3347: 'lay',\n",
       " 1984: 'eightyearold',\n",
       " 4237: 'passing',\n",
       " 3087: 'interview',\n",
       " 3724: 'michael',\n",
       " 2415: 'fox',\n",
       " 4216: 'parkinson',\n",
       " 1681: 'describing',\n",
       " 875: 'brushing',\n",
       " 5841: 'teeth',\n",
       " 5792: 'takes',\n",
       " 247: 'agony',\n",
       " 5905: 'thought',\n",
       " 5445: 'sorry',\n",
       " 1946: 'easier',\n",
       " 2801: 'help',\n",
       " 874: 'brush',\n",
       " 4074: 'older',\n",
       " 5378: 'smarter',\n",
       " 1972: 'education',\n",
       " 399: 'argument',\n",
       " 3445: 'listen',\n",
       " 6540: 'wrongness',\n",
       " 4955: 'rooted',\n",
       " 6257: 've',\n",
       " 3481: 'longer',\n",
       " 375: 'applause',\n",
       " 373: 'applaud',\n",
       " 118: '60',\n",
       " 68: '2400',\n",
       " 3083: 'interesting',\n",
       " 401: 'arizona',\n",
       " 1663: 'demographic',\n",
       " 1194: 'clap',\n",
       " 5377: 'smart',\n",
       " 1583: 'dark',\n",
       " 2970: 'identify',\n",
       " 1489: 'crazy',\n",
       " 432: 'asking',\n",
       " 96: '40',\n",
       " 6572: 'younger',\n",
       " 6437: 'whoo',\n",
       " 1046: 'celebrate',\n",
       " 976: 'cancer',\n",
       " 6348: 'ward',\n",
       " 116: '55yearold',\n",
       " 2491: 'garbage',\n",
       " 3737: 'million',\n",
       " 5947: 'times',\n",
       " 78: '28yearold',\n",
       " 4317: 'phds',\n",
       " 23: '15',\n",
       " 6521: 'worthless',\n",
       " 115: '55',\n",
       " 2135: 'experience',\n",
       " 985: 'cape',\n",
       " 1268: 'cod',\n",
       " 5711: 'summer',\n",
       " 2352: 'floating',\n",
       " 3831: 'motel',\n",
       " 4441: 'pool',\n",
       " 927: 'bus',\n",
       " 3816: 'montreal',\n",
       " 2707: 'hand',\n",
       " 3173: 'job',\n",
       " 2176: 'fair',\n",
       " 3745: 'miner',\n",
       " 3750: 'minor',\n",
       " 6171: 'understand',\n",
       " 2650: 'grown',\n",
       " 6511: 'works',\n",
       " 1754: 'dirty',\n",
       " 3132: 'jacked',\n",
       " 1447: 'county',\n",
       " 5131: 'seen',\n",
       " 2842: 'history',\n",
       " 6476: 'witnessed',\n",
       " 322: 'ancient',\n",
       " 3989: 'nixon',\n",
       " 4845: 'resign',\n",
       " 5842: 'television',\n",
       " 1195: 'clapped',\n",
       " 4508: 'president',\n",
       " 311: 'america',\n",
       " 4665: 'quit',\n",
       " 5965: 'today',\n",
       " 1759: 'disappointing',\n",
       " 6407: 'wept',\n",
       " 3050: 'insane',\n",
       " 2795: 'helicopter',\n",
       " 2340: 'flew',\n",
       " 3913: 'nation',\n",
       " 6365: 'watched',\n",
       " 3570: 'makes',\n",
       " 5181: 'sex',\n",
       " 5315: 'situation',\n",
       " 30: '17yearold',\n",
       " 1916: 'dudes',\n",
       " 5330: 'skinny',\n",
       " 659: 'best',\n",
       " 673: 'big',\n",
       " 199: 'adam',\n",
       " 376: 'apple',\n",
       " 5381: 'smelly',\n",
       " 6093: 'tshirt',\n",
       " 1951: 'easy',\n",
       " 4632: 'pussy',\n",
       " 1415: 'cool',\n",
       " 6574: 'youth',\n",
       " 2537: 'gets',\n",
       " 214: 'ads',\n",
       " 1926: 'dumpy',\n",
       " 6220: 'use',\n",
       " 2029: 'encouragement',\n",
       " 6240: 'vaguely',\n",
       " 2787: 'heavy',\n",
       " 6582: 'zero',\n",
       " 5184: 'sexual',\n",
       " 3609: 'marketplace',\n",
       " 3103: 'invisible',\n",
       " 2553: 'girls',\n",
       " 2648: 'grow',\n",
       " 4115: 'options',\n",
       " 2207: 'fast',\n",
       " 4806: 'relatively',\n",
       " 2024: 'employed',\n",
       " 6357: 'washed',\n",
       " 307: 'amazing',\n",
       " 98: '40s',\n",
       " 823: 'branch',\n",
       " 2601: 'grab',\n",
       " 2845: 'hits',\n",
       " 2645: 'ground',\n",
       " 2629: 'great',\n",
       " 1185: 'circumstances',\n",
       " 3633: 'match',\n",
       " 4730: 'real',\n",
       " 3646: 'matters',\n",
       " 5182: 'sexiest',\n",
       " 3835: 'motherfucker',\n",
       " 2405: 'formula',\n",
       " 4405: 'plus',\n",
       " 3015: 'income',\n",
       " 5516: 'squared',\n",
       " 6561: 'yep',\n",
       " 4000: 'nope',\n",
       " 6081: 'true',\n",
       " 359: 'anybody',\n",
       " 3738: 'millions',\n",
       " 6159: 'unanimously',\n",
       " 1628: 'decided',\n",
       " 3423: 'light',\n",
       " 5477: 'speed',\n",
       " 6149: 'ugly',\n",
       " 3272: 'kisses',\n",
       " 3441: 'lips',\n",
       " 5998: 'touches',\n",
       " 2521: 'genitals',\n",
       " 2052: 'entire',\n",
       " 6356: 'wash',\n",
       " 517: 'aww',\n",
       " 5430: 'solve',\n",
       " 4541: 'problem',\n",
       " 3265: 'kindness',\n",
       " 2779: 'heart',\n",
       " 3898: 'nah',\n",
       " 2457: 'fucks',\n",
       " 6487: 'women',\n",
       " 5797: 'talked',\n",
       " 1592: 'dating',\n",
       " 1451: 'courage',\n",
       " 5137: 'selection',\n",
       " 4544: 'process',\n",
       " 2098: 'evolve',\n",
       " 1160: 'choose',\n",
       " 1861: 'drag',\n",
       " 1589: 'date',\n",
       " 2732: 'happy',\n",
       " 1448: 'couple',\n",
       " 5276: 'sides',\n",
       " 3573: 'male',\n",
       " 6022: 'traditionally',\n",
       " 5468: 'speaking',\n",
       " 430: 'ask',\n",
       " 4696: 'random',\n",
       " 6330: 'walked',\n",
       " 5862: 'terrified',\n",
       " 760: 'body',\n",
       " 5844: 'telling',\n",
       " 3158: 'jerk',\n",
       " 2815: 'hi',\n",
       " 3689: 'membrane',\n",
       " 6342: 'wanna',\n",
       " 3449: 'literally',\n",
       " 2978: 'illadvised',\n",
       " 5471: 'species',\n",
       " 2121: 'existence',\n",
       " 1446: 'counts',\n",
       " 2630: 'greater',\n",
       " 5909: 'threat',\n",
       " 3694: 'men',\n",
       " 4023: 'numberone',\n",
       " 2566: 'globally',\n",
       " 2841: 'historically',\n",
       " 3046: 'injury',\n",
       " 3651: 'mayhem',\n",
       " 1774: 'disease',\n",
       " 5654: 'strokes',\n",
       " 3978: 'night',\n",
       " 4032: 'nuts',\n",
       " 5251: 'shoulders',\n",
       " 1618: 'death',\n",
       " 5568: 'statistically',\n",
       " 2687: 'halfbear',\n",
       " 2694: 'halflion',\n",
       " 2276: 'figure',\n",
       " 3704: 'mess',\n",
       " 4303: 'personality',\n",
       " 3762: 'mishmash',\n",
       " 5118: 'seconds',\n",
       " 1273: 'cohesive',\n",
       " 4700: 'ransom',\n",
       " 4007: 'note',\n",
       " 1554: 'cut',\n",
       " 3556: 'magazines',\n",
       " 726: 'blind',\n",
       " 1718: 'dick',\n",
       " 5461: 'space',\n",
       " 5927: 'thrusting',\n",
       " 3033: 'infinite',\n",
       " 1750: 'directions',\n",
       " 2895: 'hoping',\n",
       " 4259: 'pay',\n",
       " 1753: 'dirt',\n",
       " 5433: 'someplace',\n",
       " 3332: 'later',\n",
       " 3434: 'line',\n",
       " 6475: 'witness',\n",
       " 3053: 'inside',\n",
       " 4663: 'quietly',\n",
       " 5960: 'tits',\n",
       " 952: 'cakes',\n",
       " 6459: 'windows',\n",
       " 1555: 'cute',\n",
       " 5367: 'slowly',\n",
       " 2775: 'hear',\n",
       " 6344: 'wanted',\n",
       " 6470: 'wish',\n",
       " 5521: 'squeeze',\n",
       " 2336: 'flaw',\n",
       " 4380: 'planet',\n",
       " 1945: 'earth',\n",
       " 5958: 'tit',\n",
       " 4: '100',\n",
       " 1453: 'course',\n",
       " 5740: 'surgery',\n",
       " 6517: 'worry',\n",
       " 838: 'breastfeed',\n",
       " 524: 'baby',\n",
       " 2652: 'grownup',\n",
       " 6230: 'usually',\n",
       " 127: '68yearold',\n",
       " 5562: 'stately',\n",
       " 5285: 'sigourney',\n",
       " 6384: 'weaver',\n",
       " 5693: 'sucking',\n",
       " 3736: 'milk',\n",
       " 1622: 'deborah',\n",
       " 51: '200',\n",
       " 1991: 'elderly',\n",
       " 839: 'breastfeeding',\n",
       " 2030: 'end',\n",
       " 2621: 'grapes',\n",
       " 6530: 'wrath',\n",
       " 4979: 'ruin',\n",
       " 774: 'book',\n",
       " 4725: 'read',\n",
       " 1932: 'dying',\n",
       " 2550: 'girl',\n",
       " 2525: 'genre',\n",
       " 1669: 'dense',\n",
       " 2840: 'historic',\n",
       " 1202: 'classic',\n",
       " 4455: 'porny',\n",
       " 4203: 'paragraph',\n",
       " 337: 'anna',\n",
       " 3223: 'karenina',\n",
       " 5210: 'shat',\n",
       " 1128: 'chest',\n",
       " 2865: 'holy',\n",
       " 3798: 'moly',\n",
       " 6294: 'violent',\n",
       " 5220: 'shift',\n",
       " 5980: 'tone',\n",
       " 4727: 'reading',\n",
       " 3815: 'months',\n",
       " 1517: 'crowded',\n",
       " 5687: 'subway',\n",
       " 5503: 'sports',\n",
       " 5531: 'stadium',\n",
       " 5392: 'smooshed',\n",
       " 1990: 'elbow',\n",
       " 5997: 'touched',\n",
       " 5157: 'sensitivity',\n",
       " 2338: 'flesh',\n",
       " 5473: 'specifically',\n",
       " 6163: 'unbelievable',\n",
       " 1888: 'drive',\n",
       " 5788: 'tack',\n",
       " 3133: 'jacket',\n",
       " 813: 'bra',\n",
       " 4712: 'rare',\n",
       " 5996: 'touch',\n",
       " 2412: 'fourleaf',\n",
       " 1249: 'clover',\n",
       " 174: 'accident',\n",
       " 4298: 'permission',\n",
       " 173: 'access',\n",
       " 2619: 'granted',\n",
       " 5094: 'screw',\n",
       " 542: 'badly',\n",
       " 4660: 'quickly',\n",
       " 1920: 'dumb',\n",
       " 6441: 'whore',\n",
       " 3428: 'liked',\n",
       " 3666: 'meant',\n",
       " 3790: 'moderation',\n",
       " 766: 'bomb',\n",
       " 1804: 'divorce',\n",
       " 105: '48',\n",
       " 3650: 'maybe',\n",
       " 1805: 'divorced',\n",
       " 2219: 'favorite',\n",
       " 3612: 'married',\n",
       " 3676: 'meet',\n",
       " 2180: 'fall',\n",
       " 3610: 'marriage',\n",
       " 3327: 'larva',\n",
       " 5533: 'stage',\n",
       " 2731: 'happiness',\n",
       " 197: 'actually',\n",
       " 2679: 'hack',\n",
       " 5659: 'stronger',\n",
       " 4034: 'oak',\n",
       " 2181: 'falling',\n",
       " 362: 'apart',\n",
       " 2155: 'exwife',\n",
       " 3290: 'knowing',\n",
       " 1419: 'coparents',\n",
       " 2442: 'friends',\n",
       " 2212: 'father',\n",
       " 479: 'attentive',\n",
       " 2371: 'focused',\n",
       " 4858: 'responsible',\n",
       " 2585: 'goodbye',\n",
       " 4209: 'parent',\n",
       " 2199: 'fantasy',\n",
       " 6391: 'wednesday',\n",
       " 1562: 'daddy',\n",
       " 6215: 'upstairs',\n",
       " 4480: 'pour',\n",
       " 6427: 'whiskey',\n",
       " 3902: 'naked',\n",
       " 2285: 'filth',\n",
       " 1561: 'dad',\n",
       " 6281: 'videoing',\n",
       " 1573: 'dance',\n",
       " 6364: 'watch',\n",
       " 1576: 'dancing',\n",
       " 732: 'blocking',\n",
       " 6299: 'vision',\n",
       " 2855: 'holding',\n",
       " 3112: 'ipads',\n",
       " 2164: 'faces',\n",
       " 3486: 'looked',\n",
       " 4581: 'protection',\n",
       " 4557: 'program',\n",
       " 5230: 'shitty',\n",
       " 3854: 'movie',\n",
       " 2728: 'happening',\n",
       " 2241: 'feet',\n",
       " 4849: 'resolution',\n",
       " 2760: 'hd',\n",
       " 5804: 'taping',\n",
       " 6282: 'videos',\n",
       " 3765: 'missed',\n",
       " 2163: 'facebook',\n",
       " 1312: 'comments',\n",
       " 3972: 'ngaah',\n",
       " 6279: 'video',\n",
       " 6366: 'watches',\n",
       " 2416: 'frame',\n",
       " 4563: 'promise',\n",
       " 4585: 'prove',\n",
       " 5803: 'tape',\n",
       " 200: 'add',\n",
       " 50: '20',\n",
       " 3753: 'minutes',\n",
       " 4761: 'record',\n",
       " 356: 'anus',\n",
       " 4103: 'opening',\n",
       " 1241: 'closing',\n",
       " 6533: 'write',\n",
       " 212: 'adorable',\n",
       " 5547: 'star',\n",
       " 5245: 'short',\n",
       " 3525: 'lucky',\n",
       " 2545: 'gift',\n",
       " 590: 'basic',\n",
       " 4225: 'particularly',\n",
       " 304: 'ama',\n",
       " 763: 'boilerplate',\n",
       " 947: 'cable',\n",
       " 3467: 'location',\n",
       " 6071: 'trillions',\n",
       " 5694: 'sucks',\n",
       " 2161: 'eyes',\n",
       " 765: 'bolt',\n",
       " 538: 'bacon',\n",
       " 3849: 'mouth',\n",
       " 2429: 'free',\n",
       " 5256: 'shoving',\n",
       " 516: 'awkwardly',\n",
       " 361: 'anytime',\n",
       " 810: 'boyfriend',\n",
       " 558: 'balls',\n",
       " 6349: 'warm',\n",
       " 5941: 'tight',\n",
       " 889: 'buddy',\n",
       " 3390: 'lesbian',\n",
       " 3786: 'mockingbird',\n",
       " 2032: 'ending',\n",
       " 6419: 'wheel',\n",
       " 6363: 'wasted',\n",
       " 332: 'angry',\n",
       " 3912: 'nasty',\n",
       " 1676: 'depending',\n",
       " 1060: 'certain',\n",
       " 1394: 'contexts',\n",
       " 1075: 'change',\n",
       " 5169: 'set',\n",
       " 6245: 'values',\n",
       " 1578: 'dangerous',\n",
       " 1325: 'compassionate',\n",
       " 6378: 'weapon',\n",
       " 6379: 'weapons',\n",
       " 253: 'ahead',\n",
       " 5447: 'sorta',\n",
       " 1881: 'drifted',\n",
       " 3318: 'lane',\n",
       " 3027: 'indictment',\n",
       " 5434: 'son',\n",
       " 4342: 'pickup',\n",
       " 6080: 'truck',\n",
       " 4816: 'remember',\n",
       " 6557: 'yelled',\n",
       " 6458: 'window',\n",
       " 4143: 'outside',\n",
       " 3932: 'nearly',\n",
       " 1999: 'elevator',\n",
       " 3362: 'leaned',\n",
       " 698: 'bit',\n",
       " 6106: 'turn',\n",
       " 4349: 'pieces',\n",
       " 2562: 'glass',\n",
       " 5869: 'tested',\n",
       " 4780: 'reflexes',\n",
       " 6502: 'worked',\n",
       " 3839: 'motherless',\n",
       " 984: 'capable',\n",
       " 3871: 'murder',\n",
       " 3376: 'legal',\n",
       " 1393: 'context',\n",
       " 6489: 'wonder',\n",
       " 3255: 'killed',\n",
       " 3342: 'law',\n",
       " 5294: 'simple',\n",
       " 4516: 'preventing',\n",
       " 1036: 'caught',\n",
       " 3875: 'murdering',\n",
       " 3759: 'misdemeanor',\n",
       " 967: 'camera',\n",
       " 4795: 'regular',\n",
       " 3874: 'murderers',\n",
       " 5763: 'sweet',\n",
       " 6085: 'trust',\n",
       " 3677: 'meeting',\n",
       " 1498: 'creep',\n",
       " 2886: 'hooker',\n",
       " 3823: 'mormon',\n",
       " 2750: 'hate',\n",
       " 3140: 'janet',\n",
       " 1140: 'children',\n",
       " 639: 'behave',\n",
       " 1736: 'differently',\n",
       " 3574: 'mall',\n",
       " 3804: 'moms',\n",
       " 4261: 'pbbt',\n",
       " 5586: 'stepping',\n",
       " 1212: 'clean',\n",
       " 2055: 'environment',\n",
       " 4597: 'public',\n",
       " 4767: 'red',\n",
       " 545: 'bags',\n",
       " 1787: 'dispensers',\n",
       " 3474: 'logo',\n",
       " 4390: 'play',\n",
       " 4387: 'plastic',\n",
       " 5702: 'suffocate',\n",
       " 1017: 'case',\n",
       " 914: 'bunch',\n",
       " 2901: 'horrible',\n",
       " 5906: 'thoughts',\n",
       " 4967: 'row',\n",
       " 1339: 'compressed',\n",
       " 392: 'area',\n",
       " ...}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.033*\"like\" + 0.016*\"just\" + 0.016*\"know\" + 0.015*\"don\" + 0.011*\"right\" + 0.008*\"said\" + 0.007*\"got\" + 0.007*\"gonna\" + 0.006*\"think\" + 0.006*\"fucking\"'),\n",
       " (1,\n",
       "  '0.022*\"like\" + 0.016*\"just\" + 0.013*\"know\" + 0.012*\"don\" + 0.011*\"people\" + 0.010*\"right\" + 0.007*\"said\" + 0.006*\"shit\" + 0.006*\"gonna\" + 0.006*\"got\"')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term),\n",
    "# we need to specify two other parameters as well - the number of topics and the number of passes\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.013*\"like\" + 0.013*\"right\" + 0.010*\"know\" + 0.010*\"don\" + 0.009*\"just\" + 0.008*\"said\" + 0.008*\"ve\" + 0.008*\"got\" + 0.007*\"say\" + 0.007*\"joke\"'),\n",
       " (1,\n",
       "  '0.028*\"like\" + 0.018*\"just\" + 0.016*\"don\" + 0.015*\"right\" + 0.013*\"know\" + 0.011*\"people\" + 0.009*\"fucking\" + 0.009*\"gonna\" + 0.008*\"got\" + 0.008*\"shit\"'),\n",
       " (2,\n",
       "  '0.036*\"like\" + 0.018*\"know\" + 0.017*\"just\" + 0.013*\"don\" + 0.010*\"said\" + 0.007*\"people\" + 0.006*\"gonna\" + 0.006*\"right\" + 0.006*\"time\" + 0.006*\"think\"')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 3\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.017*\"right\" + 0.017*\"like\" + 0.013*\"don\" + 0.012*\"just\" + 0.011*\"fucking\" + 0.011*\"know\" + 0.009*\"went\" + 0.009*\"ve\" + 0.008*\"said\" + 0.007*\"people\"'),\n",
       " (1,\n",
       "  '0.035*\"like\" + 0.017*\"just\" + 0.016*\"know\" + 0.014*\"don\" + 0.010*\"right\" + 0.009*\"people\" + 0.009*\"said\" + 0.008*\"got\" + 0.008*\"gonna\" + 0.007*\"think\"'),\n",
       " (2,\n",
       "  '0.001*\"like\" + 0.001*\"rights\" + 0.001*\"loft\" + 0.001*\"2021\" + 0.001*\"reserved\" + 0.001*\"scraps\" + 0.001*\"just\" + 0.001*\"know\" + 0.001*\"don\" + 0.001*\"right\"'),\n",
       " (3,\n",
       "  '0.030*\"like\" + 0.020*\"just\" + 0.017*\"know\" + 0.014*\"don\" + 0.007*\"shit\" + 0.007*\"people\" + 0.007*\"gonna\" + 0.006*\"life\" + 0.006*\"cause\" + 0.005*\"ok\"')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 4\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These topics aren't looking too great. We've tried modifying our parameters. Let's try modifying our terms list as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #2 (Nouns Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>introfade the music out let’s roll hold there ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>this is dave he tells dirty jokes for a living...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ricky</th>\n",
       "      <td>hello hello how you doing great thank you wow ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bo</th>\n",
       "      <td>© 2021 scraps from the loft all rights reserved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>all right thank you thank you very much thank...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jim</th>\n",
       "      <td>ladies and gentlemen please welcome to the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john</th>\n",
       "      <td>armed with boyish charm and a sharp wit the fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hasan</th>\n",
       "      <td>© 2021 scraps from the loft all rights reserved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>ladies and gentlemen please welcome to the sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthony</th>\n",
       "      <td>thank you thank you thank you san francisco th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mike</th>\n",
       "      <td>wow hey thank you thanks thank you guys hey se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joe</th>\n",
       "      <td>ladies and gentlemen welcome joe rogan  wha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                transcript\n",
       "louis    introfade the music out let’s roll hold there ...\n",
       "dave     this is dave he tells dirty jokes for a living...\n",
       "ricky    hello hello how you doing great thank you wow ...\n",
       "bo         © 2021 scraps from the loft all rights reserved\n",
       "bill      all right thank you thank you very much thank...\n",
       "jim         ladies and gentlemen please welcome to the ...\n",
       "john     armed with boyish charm and a sharp wit the fo...\n",
       "hasan      © 2021 scraps from the loft all rights reserved\n",
       "ali      ladies and gentlemen please welcome to the sta...\n",
       "anthony  thank you thank you thank you san francisco th...\n",
       "mike     wow hey thank you thanks thank you guys hey se...\n",
       "joe         ladies and gentlemen welcome joe rogan  wha..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the cleaned data, before the CountVectorizer step\n",
    "data_clean = pd.read_pickle('data_clean.pkl')\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Prince\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>music let ’ roll hold lights lights thank i t ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>jokes living stare work profound train thought...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ricky</th>\n",
       "      <td>hello thank fuck thank welcome i m gon tonight...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bo</th>\n",
       "      <td>© scraps rights</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>thank s thank pleasure georgia area oasis t i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jim</th>\n",
       "      <td>ladies gentlemen stage mr jim jefferies thank ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john</th>\n",
       "      <td>charm wit “ snl ” writer john mulaney marriage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hasan</th>\n",
       "      <td>© scraps rights</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>ladies gentlemen stage ali hi thank hello na s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthony</th>\n",
       "      <td>thank thank people i ’ em i francisco city wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mike</th>\n",
       "      <td>wow hey thanks look insane years everyone i t ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joe</th>\n",
       "      <td>ladies gentlemen joe fck thanks phone fckface ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                transcript\n",
       "louis    music let ’ roll hold lights lights thank i t ...\n",
       "dave     jokes living stare work profound train thought...\n",
       "ricky    hello thank fuck thank welcome i m gon tonight...\n",
       "bo                                         © scraps rights\n",
       "bill     thank s thank pleasure georgia area oasis t i ...\n",
       "jim      ladies gentlemen stage mr jim jefferies thank ...\n",
       "john     charm wit “ snl ” writer john mulaney marriage...\n",
       "hasan                                      © scraps rights\n",
       "ali      ladies gentlemen stage ali hi thank hello na s...\n",
       "anthony  thank thank people i ’ em i francisco city wor...\n",
       "mike     wow hey thanks look insane years everyone i t ...\n",
       "joe      ladies gentlemen joe fck thanks phone fckface ..."
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns = pd.DataFrame(data_clean.transcript.apply(nouns))\n",
    "data_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new document-term matrix using only nouns\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Re-add the additional stop words since we are recreating the document-term matrix\n",
    "add_stop_words = ['like', 'im', 'know', 'just', 'dont', 'thats', 'right', 'people',\n",
    "                  'youre', 'got', 'gonna', 'time', 'think', 'yeah', 'said']\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
    "\n",
    "# Recreate a document-term matrix with only nouns\n",
    "cvn = CountVectorizer(stop_words=stop_words)\n",
    "data_cvn = cvn.fit_transform(data_nouns.transcript)\n",
    "data_dtmn = pd.DataFrame(data_cvn.toarray(), columns=cvn.get_feature_names())\n",
    "data_dtmn.index = data_nouns.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.010*\"day\" + 0.009*\"joke\" + 0.008*\"thing\" + 0.007*\"ve\" + 0.006*\"way\" + 0.006*\"years\" + 0.005*\"things\" + 0.005*\"dad\" + 0.005*\"cause\" + 0.005*\"baby\"'),\n",
       " (1,\n",
       "  '0.010*\"shit\" + 0.010*\"gon\" + 0.010*\"cause\" + 0.010*\"thing\" + 0.009*\"guy\" + 0.009*\"man\" + 0.008*\"life\" + 0.008*\"day\" + 0.007*\"lot\" + 0.007*\"women\"')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=2, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.015*\"shit\" + 0.011*\"gon\" + 0.010*\"life\" + 0.009*\"thing\" + 0.009*\"man\" + 0.009*\"guy\" + 0.008*\"cause\" + 0.008*\"lot\" + 0.008*\"fuck\" + 0.007*\"dude\"'),\n",
       " (1,\n",
       "  '0.011*\"day\" + 0.010*\"thing\" + 0.010*\"cause\" + 0.008*\"gon\" + 0.007*\"way\" + 0.007*\"ve\" + 0.007*\"guy\" + 0.007*\"man\" + 0.007*\"things\" + 0.006*\"house\"'),\n",
       " (2,\n",
       "  '0.001*\"thing\" + 0.001*\"day\" + 0.001*\"life\" + 0.001*\"guy\" + 0.000*\"cause\" + 0.000*\"man\" + 0.000*\"gon\" + 0.000*\"ve\" + 0.000*\"don\" + 0.000*\"years\"')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try topics = 3\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=3, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.010*\"joke\" + 0.009*\"ve\" + 0.009*\"thing\" + 0.009*\"day\" + 0.008*\"years\" + 0.006*\"things\" + 0.005*\"nuts\" + 0.005*\"jenner\" + 0.005*\"god\" + 0.005*\"don\"'),\n",
       " (1,\n",
       "  '0.013*\"shit\" + 0.009*\"guy\" + 0.009*\"man\" + 0.009*\"thing\" + 0.009*\"fuck\" + 0.009*\"gon\" + 0.008*\"day\" + 0.007*\"lot\" + 0.006*\"dude\" + 0.006*\"joke\"'),\n",
       " (2,\n",
       "  '0.013*\"cause\" + 0.010*\"gon\" + 0.010*\"life\" + 0.009*\"thing\" + 0.009*\"way\" + 0.008*\"guy\" + 0.008*\"kind\" + 0.007*\"man\" + 0.007*\"house\" + 0.007*\"kids\"'),\n",
       " (3,\n",
       "  '0.014*\"day\" + 0.011*\"women\" + 0.010*\"thing\" + 0.009*\"cause\" + 0.009*\"lot\" + 0.008*\"shit\" + 0.008*\"fuck\" + 0.007*\"gon\" + 0.007*\"guy\" + 0.007*\"ve\"')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 4 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=4, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #3 (Nouns and Adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "def nouns_adj(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
    "    return ' '.join(nouns_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>music let ’ roll hold lights lights thank much...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>dirty jokes living stare most hard work profou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ricky</th>\n",
       "      <td>hello great thank fuck thank lovely welcome i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bo</th>\n",
       "      <td>© scraps loft rights</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>right thank s thank pleasure greater atlanta g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jim</th>\n",
       "      <td>ladies gentlemen welcome stage mr jim jefferie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john</th>\n",
       "      <td>boyish charm sharp wit former “ snl ” writer j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hasan</th>\n",
       "      <td>© scraps loft rights</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>ladies gentlemen welcome stage ali wong hi wel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthony</th>\n",
       "      <td>thank san francisco thank good people surprise...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mike</th>\n",
       "      <td>wow hey thanks hey seattle nice look crazy s i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joe</th>\n",
       "      <td>ladies gentlemen joe fck san francisco thanks ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                transcript\n",
       "louis    music let ’ roll hold lights lights thank much...\n",
       "dave     dirty jokes living stare most hard work profou...\n",
       "ricky    hello great thank fuck thank lovely welcome i ...\n",
       "bo                                    © scraps loft rights\n",
       "bill     right thank s thank pleasure greater atlanta g...\n",
       "jim      ladies gentlemen welcome stage mr jim jefferie...\n",
       "john     boyish charm sharp wit former “ snl ” writer j...\n",
       "hasan                                 © scraps loft rights\n",
       "ali      ladies gentlemen welcome stage ali wong hi wel...\n",
       "anthony  thank san francisco thank good people surprise...\n",
       "mike     wow hey thanks hey seattle nice look crazy s i...\n",
       "joe      ladies gentlemen joe fck san francisco thanks ..."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns_adj = pd.DataFrame(data_clean.transcript.apply(nouns_adj))\n",
    "data_nouns_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df\n",
    "cvna = CountVectorizer(stop_words=stop_words, max_df=.8)\n",
    "data_cvna = cvna.fit_transform(data_nouns_adj.transcript)\n",
    "data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names())\n",
    "data_dtmna.index = data_nouns_adj.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.006*\"shit\" + 0.005*\"kind\" + 0.004*\"point\" + 0.004*\"mom\" + 0.004*\"hey\" + 0.004*\"ok\" + 0.004*\"jenny\" + 0.004*\"kids\" + 0.004*\"clinton\" + 0.003*\"kid\"'),\n",
       " (1,\n",
       "  '0.010*\"shit\" + 0.007*\"fuck\" + 0.006*\"fucking\" + 0.005*\"joke\" + 0.005*\"dude\" + 0.005*\"kid\" + 0.005*\"kids\" + 0.004*\"fck\" + 0.004*\"everybody\" + 0.003*\"baby\"')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=2, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.008*\"shit\" + 0.007*\"joke\" + 0.006*\"baby\" + 0.005*\"ta\" + 0.005*\"anthony\" + 0.005*\"husband\" + 0.005*\"mom\" + 0.004*\"ok\" + 0.004*\"kid\" + 0.004*\"family\"'),\n",
       " (1,\n",
       "  '0.006*\"fuck\" + 0.006*\"shit\" + 0.004*\"black\" + 0.004*\"room\" + 0.004*\"fucking\" + 0.004*\"hey\" + 0.004*\"kind\" + 0.004*\"ahah\" + 0.004*\"friend\" + 0.004*\"point\"'),\n",
       " (2,\n",
       "  '0.011*\"shit\" + 0.007*\"kids\" + 0.006*\"dude\" + 0.006*\"fck\" + 0.006*\"everybody\" + 0.006*\"kid\" + 0.005*\"fucking\" + 0.005*\"fuck\" + 0.004*\"kind\" + 0.004*\"joke\"')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 3 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=3, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try 4 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Topics in Each Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the 9 topic models we looked at, the nouns and adjectives, 4 topic one made the most sense. So let's pull that down here and run it through some more iterations to get more fine-tuned topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.009*\"joke\" + 0.005*\"mom\" + 0.005*\"parents\" + 0.004*\"hasan\" + 0.004*\"jokes\" + 0.004*\"anthony\" + 0.003*\"nuts\" + 0.003*\"dead\" + 0.003*\"tit\" + 0.003*\"twitter\"'),\n",
       " (1,\n",
       "  '0.005*\"mom\" + 0.005*\"jenny\" + 0.005*\"clinton\" + 0.004*\"friend\" + 0.004*\"parents\" + 0.003*\"husband\" + 0.003*\"cow\" + 0.003*\"ok\" + 0.003*\"wife\" + 0.003*\"john\"'),\n",
       " (2,\n",
       "  '0.005*\"bo\" + 0.005*\"gun\" + 0.005*\"guns\" + 0.005*\"repeat\" + 0.004*\"um\" + 0.004*\"ass\" + 0.004*\"eye\" + 0.004*\"contact\" + 0.003*\"son\" + 0.003*\"class\"'),\n",
       " (3,\n",
       "  '0.006*\"ahah\" + 0.004*\"nigga\" + 0.004*\"gay\" + 0.003*\"dick\" + 0.003*\"door\" + 0.003*\"young\" + 0.003*\"motherfucker\" + 0.003*\"stupid\" + 0.003*\"bitch\" + 0.003*\"mad\"')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our final LDA model (for now)\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=80)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'ali'),\n",
       " (0, 'anthony'),\n",
       " (2, 'bill'),\n",
       " (2, 'bo'),\n",
       " (3, 'dave'),\n",
       " (0, 'hasan'),\n",
       " (2, 'jim'),\n",
       " (3, 'joe'),\n",
       " (1, 'john'),\n",
       " (0, 'louis'),\n",
       " (1, 'mike'),\n",
       " (0, 'ricky')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at which topics each transcript contains\n",
    "corpus_transformed = ldana[corpusna]\n",
    "list(zip([a for [(a,b)] in corpus_transformed], data_dtmna.index))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
